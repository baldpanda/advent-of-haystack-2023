{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baldpanda/advent-of-haystack-2023/blob/main/day_1/advent_of_haystack_day_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advent of Haystack: Day 1\n",
        "_Make a copy of this Colab to start!_\n",
        "\n",
        "In this first challenge, we are going to build a RAG pipeline that answers questions based on the contents of a URL. Most of the pipeline is ready, but your task is to complete the pipeline connections üëá\n",
        "\n",
        "You should complete Step 5 of this colab.\n",
        "\n",
        "### Components to use:\n",
        "1. `LinkContentFetcher`: Expects a list of URLs and returns `ByteStream` type\n",
        "2. `HTMLToDocument`: Expects a `ByteStream` and creates `Document` type.\n",
        "3. `DocumentSplitter`: This expcects a list of `Documents` and returns a list of split, preprocessed documents.\n",
        "4. `PromptBuilder`: To define the manner we want to interact with an LLM. We use Jinja templating\n",
        "5. `GPTGenereator`: Expects a fully formed prompt and queries an OpenAI GPT model."
      ],
      "metadata": {
        "id": "5q2FHwPY-wqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Installation\n",
        "\n",
        "**Note:** There is a known issue with colab due to a version conflict error related to `llmx` which comes with Colab. You might get an `llmx` error. You can safely ignore this, or run `pip uninstall -y llmx`"
      ],
      "metadata": {
        "id": "ZALZDx-LFebK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJJ5AT3Z79e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b9041f1-6ff5-4698-aec8-7c5527627acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting haystack-ai\n",
            "  Downloading haystack_ai-2.0.0b2-py3-none-any.whl (185 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m185.7/185.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.1.2)\n",
            "Collecting lazy-imports (from haystack-ai)\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.2.1)\n",
            "Collecting openai<1.0.0 (from haystack-ai)\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (1.5.3)\n",
            "Collecting posthog (from haystack-ai)\n",
            "  Downloading posthog-3.1.0-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (6.0.1)\n",
            "Collecting rank-bm25 (from haystack-ai)\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai<1.0.0->haystack-ai) (2.31.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai<1.0.0->haystack-ai) (3.9.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog->haystack-ai)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->haystack-ai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<1.0.0->haystack-ai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<1.0.0->haystack-ai) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<1.0.0->haystack-ai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<1.0.0->haystack-ai) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0->haystack-ai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0->haystack-ai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0->haystack-ai) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0->haystack-ai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0->haystack-ai) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0->haystack-ai) (4.0.3)\n",
            "Installing collected packages: monotonic, rank-bm25, lazy-imports, backoff, posthog, openai, haystack-ai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 haystack-ai-2.0.0b2 lazy-imports-0.3.1 monotonic-1.6 openai-0.28.1 posthog-3.1.0 rank-bm25-0.2.2\n",
            "Collecting boilerpy3\n",
            "  Downloading boilerpy3-1.0.7-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: boilerpy3\n",
            "Successfully installed boilerpy3-1.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install haystack-ai\n",
        "!pip install boilerpy3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('openai_api_key')"
      ],
      "metadata": {
        "id": "qqJzSGWFPG9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Enter API keys for LLM and search providers\n",
        "Run this code and you‚Äôll be prompted to enter your OpenAI API Key. If you don‚Äôt have a key, [follow these instructions](https://help.openai.com/en/articles/4936850-where-do-i-find-my-api-key)."
      ],
      "metadata": {
        "id": "W3jwS4A_FmEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Create components"
      ],
      "metadata": {
        "id": "85pAAbaPFtZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.fetchers import LinkContentFetcher\n",
        "from haystack.components.converters import HTMLToDocument\n",
        "from haystack.components.preprocessors import DocumentSplitter\n",
        "from haystack.components.builders.prompt_builder import PromptBuilder\n",
        "from haystack.components.generators import GPTGenerator\n",
        "\n",
        "fetcher = LinkContentFetcher()\n",
        "converter = HTMLToDocument()\n",
        "splitter = DocumentSplitter(split_length=100, split_overlap=5)"
      ],
      "metadata": {
        "id": "oXMwKWzUAEaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Given the information below: \\n\n",
        "            {% for document in documents %}\n",
        "                {{ document.content }}\n",
        "            {% endfor %}\n",
        "            Question: {{ query }}. \\n Answer:\"\"\"\n",
        "\n",
        "prompt_builder = PromptBuilder(template = template)"
      ],
      "metadata": {
        "id": "qI4OaTHBa-vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = GPTGenerator(api_key = openai_api_key, model_name = \"gpt-4\")"
      ],
      "metadata": {
        "id": "98SZvedobVKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Add them to a Haystack 2.0 Pipeline"
      ],
      "metadata": {
        "id": "jnDhrfD5AayG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Pipeline\n",
        "\n",
        "pipeline = Pipeline()\n",
        "pipeline.add_component(name=\"fetcher\", instance=fetcher)\n",
        "pipeline.add_component(name=\"converter\", instance=converter)\n",
        "pipeline.add_component(name=\"splitter\", instance=splitter)\n",
        "pipeline.add_component(name=\"prompt_builder\", instance=prompt_builder)\n",
        "pipeline.add_component(name=\"llm\", instance=llm)"
      ],
      "metadata": {
        "id": "UIXUQWG5AYg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5) Connect the components\n",
        "\n",
        "Complete the pipelne connections to achieve a working pipeline that can be run\n",
        "\n",
        "**PSA:** If you are re-running this cell multiple times and you get a `PipelineConnectionError`, try restarting your Colab runtime."
      ],
      "metadata": {
        "id": "KFQOOQkaAeRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.connect(\"fetcher\", \"converter\")\n",
        "pipeline.connect(\"converter\", \"splitter\")\n",
        "pipeline.connect(\"splitter\", \"prompt_builder\")\n",
        "pipeline.connect(\"prompt_builder\", \"llm\")"
      ],
      "metadata": {
        "id": "hpDVUIhlAi0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6) Run the Pipeline"
      ],
      "metadata": {
        "id": "Y2v6GZs6Am5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_dict ={\n",
        "    \"urls\": [\"https://haystack.deepset.ai/blog/customizing-rag-to-summarize-hacker-news-posts-with-haystack2\"],\n",
        "    \"query\": \"How do you build a custom component?\"\n",
        "}\n",
        "\n",
        "\n",
        "result = pipeline.run(data={\"fetcher\": {\"urls\": query_dict[\"urls\"]}, \"prompt_builder\": {\"query\": query_dict[\"query\"]}})"
      ],
      "metadata": {
        "id": "Gbo1QwM9Ambj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['llm']['replies'][0])"
      ],
      "metadata": {
        "id": "daLZtg3gb7m0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d854dd7a-eee8-4b0e-b6ab-ee746830f406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To build a custom component in Haystack 2.0, you need to create a class with a @component decorator on the class declaration and a run function with a decorator @component.output_types(my_output_name=my_output_type) that describes what output the pipeline should expect from this component. For example, if you are creating a custom component to fetch the latest posts from Hacker News, you need to create a class called HackernewsNewestFetcher with a run function that fetches the latest posts from Hacker News.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7) Draw the Pipeline üé®\n",
        "When you run this code block, it will create a new file that will appear in the \"Files\" section of Colab (see menu tab on the side)."
      ],
      "metadata": {
        "id": "z0eEFSDYAjOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.draw(\"/content/pipeline.png\")"
      ],
      "metadata": {
        "id": "ctE6bDESAqv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C445SnjrPzSq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}